{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from io import StringIO\n",
    "from airflow import DAG\n",
    "from airflow.models import Variable\n",
    "from datetime import datetime, timedelta, time, date\n",
    "from bs4 import BeautifulSoup\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "import requests\n",
    "import pandas as pd\n",
    "from ftplib import FTP\n",
    "\n",
    "# Set default arguments for the DAG\n",
    "default_args = {\n",
    "    'owner': 'me',\n",
    "    'start_date': datetime.combine(datetime(2023, 1, 20), time(hour=23)),\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Create a new DAG\n",
    "dag = DAG(\n",
    "    'read_file_from_website_and_upload_to_s3',\n",
    "    default_args=default_args,\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "\n",
    "# Define a function that reads the file from the website\n",
    "def read_file_from_website():\n",
    "\n",
    "    # URL of the website folder containing the CSV files\n",
    "    folder_url = 'https://alphainsights.000webhostapp.com/uploads/'\n",
    "\n",
    "    # Make a request to the website to get the list of files in the folder\n",
    "    response = requests.get(folder_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract the list of files\n",
    "    files = [link.get('href') for link in soup.find_all('a')]\n",
    "\n",
    "    # create an empty dictionary to store the DataFrames\n",
    "    df_dict = {}\n",
    "    df_dict_hist = {}\n",
    "\n",
    "    # delete files from server\n",
    "    ftp = FTP(\"files.000webhost.com\")\n",
    "    ftp.login(\"alphainsights\", \"alphainsights\")\n",
    "\n",
    "    # Iterate through the list of files and read each CSV file using pandas\n",
    "    for file in files:\n",
    "        if \"csv\" in file:\n",
    "            file_url = folder_url + file\n",
    "            df = pd.read_csv(file_url)\n",
    "            name = file.split('.')[0]\n",
    "            df_dict[name] = df\n",
    "            name = name +\"-\"+ str(date.today())\n",
    "            df_dict_hist[name] = df\n",
    "            ftp.delete(\"/public_html/uploads/\"+file)\n",
    "\n",
    "    ftp.quit()\n",
    "    df_dict = {name: df.to_json() for name, df in df_dict.items()}\n",
    "    Variable.set(\"df_dict\", json.dumps(df_dict))\n",
    "    df_dict_hist = {name: df.to_json() for name, df in df_dict_hist.items()}\n",
    "    Variable.set(\"df_dict_hist\", json.dumps(df_dict_hist))\n",
    "    #return df_dict\n",
    "\n",
    "# Define a function that performs the data transformation\n",
    "def transform_data():\n",
    "    df_dict = json.loads(Variable.get(\"df_dict\"))\n",
    "    df_dict = {name: pd.read_json(df_json) for name, df_json in df_dict.items()}\n",
    "    df_dict_cleaned = {}\n",
    "    for key, df in df_dict.items():\n",
    "        # do some transformation on the DataFrame\n",
    "        if 'closed' in key:\n",
    "            df.drop(['has_company', 'has_gtin', 'average_stock', 'declared_product_catalog_size', 'declared_monthly_revenue'], axis=1, inplace=True)\n",
    "            #df['won_date'] = pd.to_datetime(df['won_date']).dt.date\n",
    "            df['won_date'] = df['won_date'].astype(str)\n",
    "            df['won_date'] = df['won_date'].str.split(\" \").str[0]\n",
    "            name = \"closed_deals\"\n",
    "            df_dict_cleaned[name] = df\n",
    "        elif 'sellers' in key:\n",
    "            df.drop(['seller_city','seller_state'], axis=1, inplace=True)\n",
    "            name = \"sellers\"\n",
    "            df_dict_cleaned[name] = df\n",
    "        elif 'customers' in key:\n",
    "            df.drop(['customer_city','customer_state'], axis=1, inplace=True)\n",
    "            name = \"customers\"\n",
    "            df_dict_cleaned[name] = df\n",
    "        else:\n",
    "            pass\n",
    "    df_dict_cleaned = {name: df.to_json() for name, df in df_dict_cleaned.items()}\n",
    "    Variable.set(\"df_dict_cleaned\", json.dumps(df_dict_cleaned))\n",
    "    #return df_dict_cleaned\n",
    "\n",
    "# Define a function that uploads the file to S3\n",
    "def upload_to_s3():\n",
    "    df_dict = json.loads(Variable.get(\"df_dict_cleaned\"))\n",
    "    df_dict = {name: pd.read_json(df_json) for name, df_json in df_dict.items()}\n",
    "    df_dict_hist = json.loads(Variable.get(\"df_dict_hist\"))\n",
    "    df_dict_hist = {name: pd.read_json(df_json) for name, df_json in df_dict_hist.items()}\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=\"AKIA3L5SBRUK2Z4GN7PL\",\n",
    "        aws_secret_access_key=\"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\",\n",
    "    )\n",
    "    s3_res = session.resource(\"s3\")\n",
    "    bucket_name = \"incremental-load-bucket\"\n",
    "    for key, df in df_dict.items():\n",
    "        csv_buffer = StringIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "        file_name = key+\".csv\"\n",
    "        s3_res.Object(bucket_name, file_name).put(Body=csv_buffer.getvalue())\n",
    "    \n",
    "    bucket_name = \"incremental-load-bucket-historicals\"\n",
    "    for key, df in df_dict_hist.items():\n",
    "        csv_buffer = StringIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "        file_name = key+\".csv\"\n",
    "        s3_res.Object(bucket_name, file_name).put(Body=csv_buffer.getvalue())\n",
    "\n",
    "# Create the Airflow tasks using the defined functions\n",
    "read_file_task = PythonOperator(\n",
    "    task_id='read_file_from_website',\n",
    "    python_callable=read_file_from_website,\n",
    "    provide_context=True,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "transform_data_task = PythonOperator(\n",
    "    task_id='transform_data',\n",
    "    python_callable=transform_data,\n",
    "    provide_context=True,\n",
    "    #op_kwargs={'df_dict': read_file_task.output()},\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "upload_to_s3_task = PythonOperator(\n",
    "    task_id='upload_to_s3',\n",
    "    python_callable=upload_to_s3,\n",
    "    provide_context=True,\n",
    "    #op_kwargs={'df_dict': transform_data_task.output()},\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Set the task dependencies\n",
    "read_file_task >> transform_data_task >> upload_to_s3_task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13ae2dc201614049a2b4c2c751d6a1c835ca6560fb2dad2f0c0ae5993fa9c2f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
